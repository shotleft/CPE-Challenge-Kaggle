{
  "cells": [
    {
      "metadata": {
        "_uuid": "d77aa2995abfe1882df75c8f5e0a9fd2c06cd6a7"
      },
      "cell_type": "markdown",
      "source": "# Key objectives\n\nThe objectives are given to us in the problem statement, and it transpires that the main challenges are not primarily in the prediction or detection space, but rather in the <i>cleaning and automation space</i> space:<br>\n\n<blockquote>Our biggest challenge is <b>automating</b> the combination of police data, census-level data, and other socioeconomic factors. Shapefiles are unusual and messy - which makes it difficult to, for instance, generate maps of police behavior with precinct boundary layers mixed with census layers. Police incident data are also very difficult to <b>normalize and standardize</b> across departments since there are no federal standards for data collection.<br>\n\n<b>Performance</b> - How well does the solution combine shapefiles and census data? <b>How much manual effort is needed?</b><br>\n\n<b>Accuracy</b> - Does the solution provide reliable and <b>accurate analysis</b>? How well does it match census-level demographics to police deployment areas?<br>\n\n<b>Approachability</b> - The best solutions should use best coding practices and have useful comments.<br></blockquote>\n\nThere are a multiplicity of issues in this messy data! So I decided to prioritize two of the main barriers to getting going:<br>\n\n<blockquote>\n<p style=\"color:MediumSeaGreen;\"><b>\n1. How do we even know what data we have, and what broad issues will we face with it?<br>\n2. How do we standardize co-ordinate reference systems (CRS) across census-level data and police-level data?</b>\n    </p></blockquote>  \n   "
    },
    {
      "metadata": {
        "_uuid": "b8ba7a9d7fdcb862711a41df32e96a1a103ac5a7"
      },
      "cell_type": "markdown",
      "source": "I'm going to skip the exploratory data analysis phase in this notebook as this has already been very well covered by (among others) [Chris Crawford](https://www.kaggle.com/crawford/another-world-famous-starter-kernel-by-chris\") and [Darren Sholes](https://www.kaggle.com/dsholes/confused-start-here), and so I'm just going to dive into \"solution mode\"... Let's get going...!"
    },
    {
      "metadata": {
        "_uuid": "fa0cf7364aa347d9adc7d4639673a2578b7c323c"
      },
      "cell_type": "markdown",
      "source": "## About projections and co-ordinate reference systems\n\nIf, like me, you are going \"co-ordinate reference systems? huh?\" I can highly recommend this quick 4-page intro which demystifies it all: http://www.ee.co.za/wp-content/uploads/legacy/Projections%20and%20coordinate.pdf"
    },
    {
      "metadata": {
        "_uuid": "65aabc9c9be9c4fa33398b6754fa854ee3c9f4c8"
      },
      "cell_type": "markdown",
      "source": "## Getting set up"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c25d2866329d8ce2d68657fca8abbf9a266e55d"
      },
      "cell_type": "code",
      "source": "# Import required libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pandas as pd\nimport geopandas as gpd\nimport geopy as gpy\nfrom geopandas import GeoDataFrame\nfrom shapely.geometry import Point\n# Set Google Maps API key for use later on - this is required to run the find_crs() function below\n# Register for free at https://cloud.google.com/maps-platform/\nread_in = pd.read_csv(\"../input/api-key/key.csv\")\napi_key = read_in[\"Key\"][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac729a6d4c469d0b74bed99f889eed31bb48c3a1"
      },
      "cell_type": "markdown",
      "source": "## External \"helper\" data sources\n\n1. <b>texas_tracts</b>: The American Community Survey (ACS) data is broken down into \"census tracts\" but the corresponding shapefiles are not supplied. Fortunately [Darren Sholes](https://www.kaggle.com/dsholes/confused-start-here) supplied us with a link: https://www.census.gov/geo/maps-data/data/cbf/cbf_tracts.html (this is one of many sets of shapefiles available on www.census.gov). You guys in the US don't realise how lucky you are to have such a rich, publicly available, nicely formatted set of census data!\n2. <b>FIPS</b>: The census tracts use what are known as FIPS codes to break down the US into states and counties. These are available for download from https://www.census.gov/geo/reference/codes/cou.html and will allow us to read which state and county data is included in any given ACS file.\n3. <b>usps_df</b>: It will also be useful to convert the US state abbreviations to full state names (especially if you are not from the US and are not already familiar with them!) - this information is available from https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations\n4. <b>projection_df</b>: And then finally a list of coordinate reference systems / spatial reference systems was obtained from  http://www.spatialreference.org/. This is used to determine which one is \"right\" for any given set of data later on. My compiled list includes 823 \"likely candidates\" but do note that for \"production purposes\" it would need to be expanded as it's not exhaustive and has been compiled for purposes of demonstrating the proposed method.\n\nYou can download the \"custom\" files that I compiled for this purpose at https://github.com/shotleft/CPE-Challenge-Kaggle."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "929cf1f1eb9ef9c49dfb0f0b4fd3e271e862e146"
      },
      "cell_type": "code",
      "source": "# Read in the Texas census tract file - a separate census tract zip file is required for each state\n# In this example I'll be sticking to Texas!\ntexas_tracts = gpd.read_file(\"../input/census-tracts-texas-censusgov/cb_2017_48_tract_500k/cb_2017_48_tract_500k.shp\")\n\n# Read in FIPS County Codes for lookup reference\nFIPS = pd.read_csv(\"../input/cpe-helper-data/FIPS_County_Codes.csv\", dtype = \"object\")\n\n# Read in the US Postal Service State Codes\nusps_df = pd.read_csv(\"../input/cpe-helper-data/USPS_State_Codes.csv\")\n\n# Read in the coordinate reference systems compiled via spatial reference.\n# Note, I selected 823 of the \"most likely to be used\" - this list could be expanded for thoroughness!\nprojection_df = pd.read_csv(\"../input/cpe-helper-data/CRS_References.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a8a3785a4200686f02c36e1ce02251f9462cefc"
      },
      "cell_type": "markdown",
      "source": "## Unpacking the need for knowing what we have\n\nDoes this sound like rather a trivial problem to be investigating? I thought so too, until I unzipped the data and got a look at the folder structure! It took a long while just to drill down into all the folders, try to work out what files we have and don't have and why - and when I started reading the data into Python with a view to exploring further I just ran into more problems! We only have 6 counties for this challenge, which is still manageable, but imagine once we extend this project to all 52 states with all the counties each contains! Here is a quick glimpse into the folder structure:\n\n![](https://shotlefttodatascience.files.wordpress.com/2018/10/folder_img2.jpg)\n\nSome of the key issues I uncovered were:<br>\n\n- The departments are given as numbers? What do those numbers actually represent?\n- In some cases we are given a \"use of force\" incident file, and in others not which affects what we'll be able to analyse and plot, it would be nice to know without opening every folder to look!\n- After [some reading](https://en.wikipedia.org/wiki/Shapefile) it becomes apparent that the following shapefile types are <i>required</i> if we are to make sense of given shapefile data: \"shp\", \"shx\", \"dbf\", \"prj\" - but are they present in all cases?\n- After examining the ACS data provided I also discovered that in some cases the wrong region has been provided in a category, for example the race-sex-age data supplied for Travis County actually contains data for Hill County, eeish!\n\nFortunately, what IS consistent is the directory structure, directory naming conventions and placement of files within directories so we can use this to create a function to read and summarize \"what we have\" as a starting point..."
    },
    {
      "metadata": {
        "_uuid": "b89bb793d452944a98c264c076c33f77aeffdae8"
      },
      "cell_type": "markdown",
      "source": "### Assembling our starter data structures\n\nKnowing that we have a consistent folder structure allows us to read in which folders and files we have."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "818d2e2fd50371d9e6c29e85c0ee1f07160cb7f3"
      },
      "cell_type": "code",
      "source": "# Read in initial list of departments available (the top level of our directory structure)\ndept_list = [d for d in next(os.walk(\"../input/data-science-for-good/cpe-data\"))[1] if not d.startswith((\".\", \"_\"))]\n\n# Read in directories per department (the next level down)\ndept_dir_dict = {}\nfor dept in dept_list:\n    depts = [d for d in next(os.walk(os.path.join(\"../input/data-science-for-good/cpe-data\", dept)))[1] if not d.startswith((\".\", \"_\"))]\n    dept_dir_dict.update({dept:depts})\n    \n# Read in sub-directories per directory (one more level down)\ndept_subdir_dict = {}\nfor dept in dept_dir_dict:\n    for subdir in dept_dir_dict[dept]:\n        subsubdir = [d for d in next(os.walk(os.path.join(\"../input/data-science-for-good/cpe-data\", dept, subdir)))[1] if not d.startswith((\".\", \"_\"))]\n        dept_subdir_dict.update({subdir:subsubdir})\n        \n# For each department we expect ACS data - get a list of ACS directories\nsub_dir_acs = []\nfor i in range(len(dept_list)):\n    sub = [s for s in dept_dir_dict[dept_list[i]] if \"ACS\" in s][0]\n    sub_dir_acs.append(sub)\n    \n# Within each ACS directory we expect 5 sub-folders for each category of data - get a list of ACS sub-directories\nsub_dir_acs_det = []\nfor i in range(len(sub_dir_acs)):\n    subsub = [s for s in dept_subdir_dict[sub_dir_acs[i]]]\n    sub_dir_acs_det.append(subsub)\nfor i in range(len(sub_dir_acs_det)):\n    sub_dir_acs_det[i].sort()\n    \n# Create a dictionary that can be used to reference which type of ACS data we want to retrieve\nacs_dict = {\"education\" : 0, \"education25\" : 1, \n           \"housing\" : 2, \"poverty\" : 3, \"rsa\" : 4}\n\n# And create a dictionary to go back - from dictionary number to ACS descriptions\ninv_acs_dict = {v: k for k, v in acs_dict.items()}\n    \n# For each department we expect shapefile data - get a list of shapefile directories\nsub_dir_shp = []\nfor i in range(len(dept_list)):\n    sub = [s for s in dept_dir_dict[dept_list[i]] if \"hape\" in s][0]\n    sub_dir_shp.append(sub)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ba061c03b25cc600e5064cc4f06b3c3509509f6"
      },
      "cell_type": "markdown",
      "source": "### Create functions to read in required data\n\nSooner or later we'll need to be reading in the actual data so that we can plot it and analyse it - so I've written some functions to do this for ease of use down the line... Each is documented via docstring below:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ae73fe0aa28e5b5da2354e10c2ad8636bb9fff1"
      },
      "cell_type": "code",
      "source": "def read_uoffile(dept):\n    \"\"\"This function reads in UOF data for the requested department.\n    Returns a Pandas dataframe after reading in the relevant .csv file.\"\"\"\n    path = os.path.join(\"../input/data-science-for-good/cpe-data\", \n                        dept_list[int(dept)])\n    file = [f for f in os.listdir(os.path.join(\"../input/data-science-for-good/cpe-data\", \n                                               dept_list[int(dept)])) if \"UOF\" in f][0]\n    full_path = os.path.join(path, file)\n    \n    df = pd.read_csv(full_path)\n    return df\n\ndef read_shapefile(dept):\n    \"\"\"This function reads in police shapefile data for the requested department.\n    Returns a GeoPandas dataframe after reading in the relevant \"shp\", \"shx\", \"dbf\" and \"prj\" files.\"\"\"\n    path = os.path.join(\"../input/data-science-for-good/cpe-data\", dept_list[int(dept)], \n                        sub_dir_shp[int(dept)])\n    file = [f for f in os.listdir(os.path.join(\"../input/data-science-for-good/cpe-data\", \n                                               dept_list[int(dept)], \n                                               sub_dir_shp[int(dept)])) if \".shp\" in f][0]\n    full_path = os.path.join(path, file)\n    \n    gdf = gpd.read_file(full_path)\n    return gdf\n\ndef read_acsfile_key(dept,category):\n    \"\"\"This function reads in ACS data for the requested department and category.\n    Returns a FIPS key (where digits 0-1 = State, digits 2-4 = County).\"\"\"\n    path = os.path.join(\"../input/data-science-for-good/cpe-data\", \n                        dept_list[int(dept)], \n                        sub_dir_acs[int(dept)], \n                        sub_dir_acs_det[int(dept)][int(category)])\n    file = [f for f in os.listdir(os.path.join(\"../input/data-science-for-good/cpe-data\", \n                        dept_list[int(dept)], \n                        sub_dir_acs[int(dept)], \n                        sub_dir_acs_det[int(dept)][int(category)])) if \"ann\" in f][0]\n    full_path = os.path.join(path, file)\n    \n    df = pd.read_csv(full_path).head()\n    FIPS_info = df.iloc[1, 1]\n    return FIPS_info\n\ndef read_acsfile(dept,category):\n    \"\"\"This function reads in ACS data for the requested department and category.\n    Returns a Pandas dataframe after reading in the relevant .csv file.\"\"\"\n    path = os.path.join(\"../input/data-science-for-good/cpe-data\", \n                        dept_list[int(dept)], \n                        sub_dir_acs[int(dept)], \n                        sub_dir_acs_det[int(dept)][int(category)])\n    file = [f for f in os.listdir(os.path.join(\"../input/data-science-for-good/cpe-data\", \n                        dept_list[int(dept)], \n                        sub_dir_acs[int(dept)], \n                        sub_dir_acs_det[int(dept)][int(category)])) if \"ann\" in f][0]\n    full_path = os.path.join(path, file)\n    \n    df = pd.read_csv(full_path)\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9fbe64b35db02b004db1892b6b599707188385cd"
      },
      "cell_type": "markdown",
      "source": "### Create functions to check the data\nAnd then we create a set of functions to check the data - again each is documented via docstring below:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fbb121417cf474c3c680317fd940064a9fed5d4"
      },
      "cell_type": "code",
      "source": "def check_shapefiles():\n    \"\"\"This function checks availability of required mandatory shapefiles by department.\n    Returns a list of lists containing shapefile extensions.\"\"\"\n    mandatory_files = [\"shp\", \"shx\", \"dbf\", \"prj\"]\n    shapefile_check = []\n\n    for i in range(len(dept_list)):\n        row_check = []\n        for file in mandatory_files:\n            try:\n                if [f for f in os.listdir(os.path.join(\"../input/data-science-for-good/cpe-data\", dept_list[i], sub_dir_shp[i])) if file in f][0]:\n                    row_check.append(file)\n            except:\n                pass\n        shapefile_check.append(row_check)\n    return shapefile_check\n\ndef check_uoffiles():\n    \"\"\"This function checks availability of required use of force files by department.\n    Returns a list UOF if available or None if not available.\"\"\"\n    uof_var = \"UOF\"\n    uoffile_check = []\n\n    for i in range(len(dept_list)):\n        try:\n            if [f for f in os.listdir(os.path.join(\"../input/data-science-for-good/cpe-data\", dept_list[i])) if uof_var in f][0]:\n                uoffile_check.append(uof_var)\n        except:\n            uoffile_check.append(\"None\")\n    return uoffile_check\n\ndef check_acsfiles():\n    \"\"\"This function retrieves the FIPS codes per ACS file by department.\n    Returns a list of lists containing FIPS data.\n    \"\"\"\n    FIPS_grid = []\n    for j in range(5):\n        cat_check = []\n        for i in range(len(dept_list)):\n            FIPS_info = read_acsfile_key(i,j)\n            cat_check.append(FIPS_info)\n        FIPS_grid.append(cat_check)\n    return FIPS_grid\n\ndef color_exceptions(val):\n    \"\"\"Checks for defined exception values and highlights them in red.\"\"\"\n    color = 'red' if (val == \"None\") or (val == False) or (val == ['shp', 'shx', 'dbf']) else 'black'\n    return 'color: %s' % color\n\ndef assemble_overview():\n    \"\"\"This function assembles an overview of the data provided and highlights any key issues,\n    including 1) missing *.prj files 2) missing use of force files 3) inconsistent ACS data\"\"\"\n    \n    # Let's run the check_shapefiles and get a list of shapefiles available per department\n    dept_shapes = check_shapefiles()\n\n    # Let's run the check_uoffiles and get a list of uoffiles available per department\n    dept_uofs = check_uoffiles()\n\n    # Let's run the check_acsfiles and get a list of acsfiles available per department\n    cat_acs = check_acsfiles()\n    \n    overview = pd.DataFrame({\"dept\": dept_list, \n                             \"uofs\": dept_uofs, \n                             \"shapes\": dept_shapes, \n                             inv_acs_dict[0] : cat_acs[0],\n                             inv_acs_dict[1] : cat_acs[1],\n                             inv_acs_dict[2] : cat_acs[2],\n                             inv_acs_dict[3] : cat_acs[3],\n                             inv_acs_dict[4] : cat_acs[4]})\n    overview[\"state\"] = overview[\"education\"].str[0:2]\n    overview[\"education\"] = overview[\"education\"].str[0:5]\n    overview[\"education25\"] = overview[\"education25\"].str[0:5]\n    overview[\"housing\"] = overview[\"housing\"].str[0:5]\n    overview[\"poverty\"] = overview[\"poverty\"].str[0:5]\n    overview[\"rsa\"] = overview[\"rsa\"].str[0:5]\n\n    # Get county text\n    county = []\n    for i in overview.index:\n        county_val = FIPS.loc[((FIPS[\"state_code\"] == overview.loc[i, \"education\"][0:2]) & (FIPS[\"fips_county\"] == overview.loc[i, \"education\"][2:5])), \"description\"].values[0]\n        county.append(county_val)\n    overview[\"county\"] = county\n\n    # Get state text keys\n    state = []\n    for i in overview.index:\n        state_val = FIPS.loc[(FIPS[\"state_code\"] == overview.loc[i, \"state\"]), \"state\"].values[0]\n        state.append(state_val)\n    overview[\"state\"] = state\n\n    # Re-assemble the df with fields in required order\n    overview = overview[['dept', 'state', 'county', 'uofs', 'shapes', 'education', 'education25', 'housing',\n           'poverty', 'rsa']]\n\n    # Add a column reflecting if there are any issues with the ACS data\n    acs_check = []\n    for i in overview.index:\n        eval = overview.loc[i, \"education\"] == overview.loc[i, \"education25\"] == overview.loc[i, \"housing\"] == overview.loc[i, \"poverty\"] == overview.loc[i, \"rsa\"]\n        acs_check.append(eval)\n    overview[\"acs_ok\"] = acs_check\n\n    # And finally, let's color any exceptions found and present our overview\n    data_overview = overview.style.applymap(color_exceptions)\n    \n    return data_overview",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9f48957ea8c4dc58895ff664f6b6e7372cafb52"
      },
      "cell_type": "markdown",
      "source": "### The result - what do we have?\n\nSo the main aim is to end up with a single neat Pandas df, which gives us an overview of our files, and so of the key issues at a glance:\n\n- What county / state combinations does each Dept file represent?\n- Has a use of force (uof) file been provided or not?\n- In the provided shapefile data are any of the 4 key files missing?\n- In the ACS data were the files provided consistently for the same county?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d99ba90bb50fd670dc5592d2480d60a7f431294d"
      },
      "cell_type": "code",
      "source": "overview = assemble_overview()\noverview",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4fd18f169caa71a2fc973954b269d5f611f2875"
      },
      "cell_type": "markdown",
      "source": "In the above output we see immediately:\n\n1. We have only been given use of force data for 3 of our 6 counties: Travis, Mecklenburg and Dallas.\n2. The shapefile data from Travis is missing a \"prj\" file so we're going to need to identify the correct coordinate reference system before we can even thinking of analysing or plotting that data further.\n3. And for both Travis and Worcester we have troublesome ACS data - notice that for Travis all entries are \"48453\", except for rsa (race-sex-age) where we've been given the data for FIPS county \"48217\". Similarly for Worcester the rsa data is for a different county.\n\nOnce we have data available for all 52 states, this quick analysis will give us a good idea of where we can dive in and start working with the data, and where we need to focus on further data preparation efforts before we begin.<br>\n\nDallas looks reasonably hassle-free and complete, so let's do some basic plotting using Dept_37-00049 as an example, just to get a feel for the process...<br>\n\n(Note that when reading in data I'll be using the row indices to refer to requested departments and the columns education through rsa (0 through 4) will refer to the requested ACS data)"
    },
    {
      "metadata": {
        "_uuid": "2f2aab30dba268ceafa9076010f606ab62b22e69"
      },
      "cell_type": "markdown",
      "source": "## Basic plotting - Dallas\n<b>Objective</b>: We have 3 separate datasets that we want to overlay: American Community Survey data, police shapefile data, and use of force data. The ACS data is consistently plotted using CRS = epsg:4269, so for my purposes I'm going to use that as my base standard - everything else must transformed to conform to this standard!"
    },
    {
      "metadata": {
        "_uuid": "519ee46aa74f5f1512e4ce478a67e7ce5c547347"
      },
      "cell_type": "markdown",
      "source": "### Create functions required to prepare for plotting\nOur data needs some preparation before it can be plotted - again each is documented via docstring below:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9430ca404d4317cd60f3f0884a25b6d9db78ddd"
      },
      "cell_type": "code",
      "source": "# Note that it is not currently possible to run this function on Kaggle currently due \n# to the limitation around rtree https://github.com/Kaggle/docker-python/issues/108\ndef make_geodf_uof_acs(uoffile, long_col, lat_col, acsfile):\n    \"\"\"Converts the specified uoffile with longitude and latitude to a GeoPandas dataframe,\n    and ensures that only UOF data within the supplied ACS file boundaries is displayed.\"\"\"\n    geometry = [Point(xy) for xy in zip(uoffile[long_col].astype(\"float\"), \n                                    uoffile[lat_col].astype(\"float\"))]\n    uoffile = GeoDataFrame(uoffile, crs = \"epsg:4269\", geometry = geometry)\n    uoffile = gpd.sjoin(uoffile, acsfile, how=\"inner\", op='intersects')\n    return uoffile\n\ndef make_geodf_uof(df, x_col, y_col):\n    \"\"\"Converts a pandas df to a GeoPandas df, using the specified x and y data to create 'geometry'.\"\"\"\n    geometry = [Point(xy) for xy in zip(df[x_col].astype(\"float\"), \n                                    df[y_col].astype(\"float\"))]\n    df = GeoDataFrame(df, geometry = geometry)\n    return df\n\ndef check_crs(shapefile):\n    \"\"\"The ACS data uses epsg:4269 as a standard, so our aim is to standardize all map co-ordinates on epsg:4269\n    so that no matter where the data comes from it can be analysed and plotted within the same projection.\n    Returns an evaluation of the relevant shapefile with recommendation where required.\"\"\"\n    target = 'epsg:4269'\n    if shapefile.crs == {}:\n        print(\"no initial projection - use find_crs() to find the correct projection\") # see later in notebook\n    elif shapefile.crs == target:\n        print(\"no problem - ready for mapping\")\n    else:\n        print(\"requires conversion - use conv_crs() to fix\")\n        \ndef conv_crs(shapefile):\n    \"\"\"Converts the specified shapefile from one CRS to our standard epsg:4269\"\"\"\n    shapefile = shapefile.to_crs(epsg='4269')\n    return shapefile",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b2ec1c23c8d646f1e705367e1d6dc289114ab32"
      },
      "cell_type": "markdown",
      "source": "### Preparing our plot data\nThe ACS data is quite nice and consistent - the same can't be said for the police data - this is another whole area of challenge! For now I'll manually do some identification and cleaning of these files just so we can check if our overlay objective is being achieved as expected."
    },
    {
      "metadata": {
        "_uuid": "246dbfa04f20816d6d8c5aa009e2fc897c5a6d0c"
      },
      "cell_type": "markdown",
      "source": "#### First the ACS data..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a357540308795b371b1f84508a3b47b85335e2da"
      },
      "cell_type": "code",
      "source": "# Read in the ACS file for Dallas (3), Poverty (3) as per overview table above\ndallas_acs = read_acsfile(3, 3)\n\n# Rename the GEO.id column to AFFGEOID so it matches to the corresponding tract file column name\ndallas_acs.rename(columns = {\"GEO.id\": \"AFFGEOID\"}, inplace = True)\n\n# Merge the ACS file for Dallas, Poverty with the Texas tracts data\ndallas_acs = dallas_acs.merge(texas_tracts, on = \"AFFGEOID\")\n\n# And then convert the resulting df to a Gdf\ndallas_acs = GeoDataFrame(dallas_acs, crs = \"epsg:4269\", geometry = dallas_acs[\"geometry\"])\n\n# And then let's look at the resulting output\nfig, ax = plt.subplots(figsize = (12, 12))\n# Plot the ACS data\nax = dallas_acs.plot(ax = ax, color = \"paleturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\n# Add a title\nfig.suptitle('Dallax Texas, Census Tracts', x = 0.5, y = 0.89)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a63822addda7b0d998957be6d71becb2ae3c452"
      },
      "cell_type": "markdown",
      "source": "#### And then the police file shape data..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa54fb44a1a54399fae96971db05f2a5394e72b8"
      },
      "cell_type": "code",
      "source": "# Let's read in the police shapefile data given\ndallas_shapes = read_shapefile(3)\ndallas_shapes.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4dcd909854fa6d5c258e524b6873182a53b9f84"
      },
      "cell_type": "code",
      "source": "# Let's check how the projection of our police shapefile lines up against our agreed standard for ACS\ncheck_crs(dallas_shapes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d1fc7df66f3a411aec0379ed84bc5b371258f78"
      },
      "cell_type": "code",
      "source": "# If we print the crs values we can see they are completely different\nprint(dallas_acs.crs, \"vs\", dallas_shapes.crs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "937474cb3d55abcca5385d068a08d118bd66ce25"
      },
      "cell_type": "code",
      "source": "# And we can see visually that it's problematic by trying to plot the ACS data and the police shapefile data together - \n# we get a plot with nothing in it!\nfig, ax = plt.subplots(figsize = (10, 10))\nax = dallas_acs.plot(ax = ax, color = \"paleturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\ndallas_shapes.plot(ax = ax, color = \"none\", edgecolor = \"b\", linewidth = 1.5)\nfig.suptitle('Dallas Texas, Census Tracts with Police Districts', x = 0.5, y = 0.88)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "fdfe080fb19e4769242758ebfa659448d3d01d02"
      },
      "cell_type": "code",
      "source": "# That's OK though because remember we do have an initial CRS to work with so \n# let's use our function to convert to our standard epsg:4269\ndallas_shapes = conv_crs(dallas_shapes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5e762c8acaf75e5c84ca0f617dafdc0f8aa39655"
      },
      "cell_type": "code",
      "source": "# And if we plot ACS data and police shapefile data they are now lining up nicely\nfig, ax = plt.subplots(figsize = (12, 12))\n# Plot the ACS data\nax = dallas_acs.plot(ax = ax, color = \"paleturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\n# Add the shapefile data\ndallas_shapes.plot(ax = ax, color = \"none\", edgecolor = \"b\", linewidth = 1.5)\n# Add a title\nfig.suptitle('Dallas Texas, Census Tracts with Police Districts', x = 0.5, y = 0.88)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c64a824f3509ad1dae6178ad40016d44368c650e"
      },
      "cell_type": "markdown",
      "source": "#### And then the use of force data..."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "4173e3aa9b222aa3ddd58702ceb513fda28cc166"
      },
      "cell_type": "code",
      "source": "# Now let's read in our UOF data for Dallas - in this case we're lucky as the incident co-ordinates are given in \n# latitude and longitude already...\ndallas_uof = read_uoffile(3)\ndallas_uof.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "54729addd5b0fcd04a0196fb4c5f8fc8a97f9ad5"
      },
      "cell_type": "code",
      "source": "# We need to do a little cleaning of non-null, non-numeric and data type values before proceeding\ndallas_uof.dropna(subset = [\"LOCATION_LATITUDE\"], inplace=True)\ndallas_uof.dropna(subset = [\"SUBJECT_RACE\"], inplace=True)\ndallas_uof.drop([0], inplace = True)\ndallas_acs[\"HC02_EST_VC01\"] = dallas_acs[\"HC02_EST_VC01\"].astype(\"float\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "fb986d088e3c4105fa61937673275b989b6cf345"
      },
      "cell_type": "code",
      "source": "# And now let's convert our use of force data to a GeoPandas dataframe\ndallas_uof = make_geodf_uof(dallas_uof, \"LOCATION_LONGITUDE\", \"LOCATION_LATITUDE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5df8e2ff869ce00d1f707cfae656a76c829e6942"
      },
      "cell_type": "code",
      "source": "# It would be nice to add labels to our data as a finishing touch so we'll get \"representative points\"\n# for each shape which we'll use to plot our labels later on\ndallas_shapes['coords'] = dallas_shapes['geometry'].apply(lambda x: x.representative_point().coords[:])\ndallas_shapes['coords'] = [coords[0] for coords in dallas_shapes['coords']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "262bfac12e89d11d68e593b708dc9e5f12415d3b"
      },
      "cell_type": "code",
      "source": "# Let's get some basic statistics for HC02_EST_VC01 (\"Below poverty level; Estimate; Population for whom \n# poverty status is determined\") - we'll use this to highight rich vs poor levels on our map\ndallas_acs[\"HC02_EST_VC01\"].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f3fa1e7e928139d962ceec3dafe777c3c99d6ac"
      },
      "cell_type": "markdown",
      "source": "#### And finally our plot\nI'm quite convinced there are many ways to automate this as well (this would be my next challenge!) but for now let's get the job done :). It would also be nice to use the spatial join functionality mentioned above to ensure that only incidents within the census tract boundaries are plotted if desired (hopefully Kaggle will soon advise that rtree can be installed on the platform!)."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "f6c1539298478cb7461c0508c87a30365da0651e"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize = (12, 12))\n\n# Plot the ACS data by poverty level\nax = dallas_acs.plot(ax = ax, color = \"c\", edgecolor = \"darkgrey\", linewidth = 0.5)\ndallas_acs[dallas_acs[\"HC02_EST_VC01\"] >= dallas_acs[\"HC02_EST_VC01\"].describe()[\"50%\"]].plot(ax = ax, color = \"mediumturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\ndallas_acs[dallas_acs[\"HC02_EST_VC01\"] >= dallas_acs[\"HC02_EST_VC01\"].describe()[\"75%\"]].plot(ax = ax, color = \"paleturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\n\n# Add the shapefile data\ndallas_shapes.plot(ax = ax, color = \"none\", edgecolor = \"b\", linewidth = 1.5)\n\n# Add the use of force data\ndallas_uof.plot(ax = ax, color = \"orangered\", alpha = 0.5, markersize = 10)\n\n# Some labels would also be nice!\ntexts = []\nfor i in dallas_shapes.index:\n    text_item = [dallas_shapes[\"coords\"][i][0] + 0.015, \n                  dallas_shapes[\"coords\"][i][1] + 0.015, \n                  dallas_shapes[\"Name\"][i]]\n    texts.append(text_item)\n    plt.text(texts[i][0], texts[i][1], texts[i][2], color = \"black\")\n\n# And finally there are a lot of colours so a key will be useful   \nlow_line = matplotlib.lines.Line2D([], [], color='c',markersize=120, label='poverty - low')\nmed_line = matplotlib.lines.Line2D([], [], color='mediumturquoise',markersize=120, label='poverty - medium')\nhigh_line = matplotlib.lines.Line2D([], [], color='paleturquoise', markersize=120, label='poverty - high')\npolice_line = matplotlib.lines.Line2D([], [], color='b', markersize=120, label='police precincts')\nuof_line = matplotlib.lines.Line2D([], [], color='orangered', markersize=120, label='use of force')\nhandles = [low_line, med_line, high_line, police_line, uof_line]\nlabels = [h.get_label() for h in handles] \nax.legend(handles=handles, labels=labels, fontsize = 10, loc='lower right', shadow = True)\n\n# Add a title\nfig.suptitle('Dallas Texas, Use of Force by Poverty Levels and Police District', x = 0.5, y = 0.88)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8aefc184cacc40a6421348dd6a4e39f75595b00f"
      },
      "cell_type": "markdown",
      "source": "## Sorting out Travis!\n<b>Objective</b>: Travis was the department where no *prj file was available so we don't know the current projection and we need to find the right coordinate reference system before we can proceed with plotting the police-supplied data onto the census data."
    },
    {
      "metadata": {
        "_uuid": "802610136aaf7dc539882d5067190cb2d7441bf8"
      },
      "cell_type": "markdown",
      "source": "#### First the ACS data..."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "04460832fb825b0ed4eb610cbbc942b45d687c5a"
      },
      "cell_type": "code",
      "source": "# Read in the ACS file for Travis(4), Poverty (3) as per overview table above\ntravis_acs = read_acsfile(4, 3)\n\n# Rename the GEO.id column to AFFGEOID so it matches to the corresponding tract file column name\ntravis_acs.rename(columns = {\"GEO.id\": \"AFFGEOID\"}, inplace = True)\n\n# Merge the ACS file for Dallas, Poverty with the Texas tracts data\ntravis_acs = travis_acs.merge(texas_tracts, on = \"AFFGEOID\")\n\n# And then convert the resulting df to a Gdf\ntravis_acs = GeoDataFrame(travis_acs, crs = \"epsg:4269\", geometry = travis_acs[\"geometry\"])\n\n# And then let's look at the resulting output\n\nfig, ax = plt.subplots(figsize = (12, 12))\nax = travis_acs.plot(ax = ax, color = \"paleturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\nfig.suptitle('Travis Texas', x = 0.5, y = 0.84)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8807a71a7dfae817dda1862baeddb65489fefe98"
      },
      "cell_type": "markdown",
      "source": "#### And then the police file shape data..."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "052a6ad1f98423fad713363039afc4efcf21ad4e"
      },
      "cell_type": "code",
      "source": "# Let's read in the police shapefile data given\ntravis_shapes = read_shapefile(4)\ntravis_shapes.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "8743a0b1b22fe47e407a71d395167f7c54a22528"
      },
      "cell_type": "code",
      "source": "# Let's check how the projection of our police shapefile lines up against our agreed standard for ACS\ncheck_crs(travis_shapes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d0edb19b58558f7e552df4b1e0828f5442e407f9"
      },
      "cell_type": "code",
      "source": "# If we print the crs values we can see that there is simply NO crs data available for Travis ({})\nprint(travis_acs.crs, \"and\", travis_shapes.crs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e3afacb3da8e41cf44cd5587ac93a194e1eb9833"
      },
      "cell_type": "markdown",
      "source": "#### So let's turn to the use of force data...\nBecause <i>this</i> data comes with addresses which are the key to helping us determine the right CRS to use..."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "3f6d66138ca94c025274162ee377b3fd8acf3c94"
      },
      "cell_type": "code",
      "source": "# Now let's read in our UOF data for Travis - we seem to have 2 Y co-ordinates(!) as well as longitude and latitude -\n# yikes!\ntravis_uof = read_uoffile(4)\ntravis_uof.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5c3c75cb55f669c85ec89a20bc54c7b751a2c7d0"
      },
      "cell_type": "code",
      "source": "# A closer look at the data reveals that latitude and longitude are seldom given so there are \n# too many null values to be useful. The 2 \"Y co-ordinates\" are actually X and Y, they've just been\n# mis-labelled by whatever process they went through previously. We also observe that our X and Y are\n# certainly not latitude or longitude so we're going to have to find a suitable projection,\n# AND we are given physical address data in this file, and this is going to provide us with the means\n# to determine the right projection (LOCATION_FULL_STREET_ADDRESS_OR_INTERSECTION)\ntravis_uof.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "57885d2ad6a087c9e5949ad124f72041aea75477"
      },
      "cell_type": "code",
      "source": "# Let's re-name our columns for ease of use\ntravis_uof.rename(columns = {\"Y_COORDINATE\" : \"X_COORDINATE\", \"Y_COORDINATE.1\" : \"Y_COORDINATE\"}, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "dd2858b95a35066e9b70eb8655a0dc18874092d8"
      },
      "cell_type": "code",
      "source": "# And then do a little cleaning of non-null, non-numeric values\ntravis_uof.dropna(subset = [\"X_COORDINATE\"], inplace=True)\ntravis_uof.drop([0], inplace = True)\ntravis_uof.drop(list(travis_uof[travis_uof[\"X_COORDINATE\"] == \"-\"].index), inplace = True)\ntravis_uof.drop([2094], inplace = True)\ntravis_acs[\"HC02_EST_VC01\"] = travis_acs[\"HC02_EST_VC01\"].astype(\"float\")\ntravis_uof.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21b68efebd9d6ec0ab23a6f3bca8657772a10fc5"
      },
      "cell_type": "markdown",
      "source": "### Create a function to \"detect\" the best fit CRS\nWhatever inconsistencies there are in the police data, the one thing that DOES seem to be consistently supplied is the physical address of the incident. This is what will enable us to determine the correct CRS to use for the data - the docstring below explains the principle behind this function. "
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "c96089ca4f529d393c72337e5703a379ebc0a3a2"
      },
      "cell_type": "code",
      "source": "def find_crs(rand_geodf_source):\n    \"\"\"Selects 3 random addresses from the specified GeoPandas df and retrieves latitude and longitude for them\n    via Google maps API (this is then transformed to our standard epsg:4269. Selects a list of 'likely' projections\n    - just based on State for now for demonstration purposes - and then tests what happens when we convert from that \n    CRS to our standard CRS. The CRS with the least difference in distance between our data and Google is deemed \n    the closest match and can be used for conversion.\"\"\"\n    # Now let's get corresponding locations for these addresses from Google\n    rand_google = gpd.tools.geocode(rand_addresses.values, provider=\"googlev3\", api_key = api_key)\n    # And then convert to the standard projection we've decided upon\n    rand_google.to_crs(epsg='4269')\n    \n    # Let's create a df where we'll store our evaluation data\n    rand_eval = pd.DataFrame(rand_geodf_source[\"address\"])\n    \n    # Get a list of projections to try\n    projection_tries = projection_df[projection_df[\"PROJ Description\"].str.contains(usps_df.loc[usps_df[\"USPS\"] == rand_geodf_source[\"state\"][0], \"State\"].values[0])]\n    projection_tries.reset_index(drop = True, inplace = True)\n        \n    for i in range(len(projection_tries)):\n        # First we make a copy of our source data to work on\n        rand_geodf = rand_geodf_source.copy()\n\n        # Let's now set the crs to the first one we want to try\n        rand_geodf.crs = {'init' : projection_tries[\"PROJ\"][i]}\n        # And then convert to our standard\n        rand_geodf = rand_geodf.to_crs(epsg='4269')\n\n        # And let's store the outcomes of our first test\n        rand_eval[projection_tries[\"PROJ\"][i]] = rand_geodf.distance(rand_google)\n    \n    # Find the mean of the values for each column\n    rand_eval.loc['avg'] = rand_eval.mean()\n    \n    # Find the best fit\n    answer = rand_eval.loc['avg'].dropna().sort_values().index[0]\n    \n    return answer, rand_eval",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "efc80e25291e3fb3d77d4b1c3d5a6432d2522255"
      },
      "cell_type": "code",
      "source": "# Now we can randomly pick 3 addresses we'll use to validate on\nrand_addresskeys = list(np.random.randint(1,len(travis_uof), 3))\n\n# We need the full address for best geo-coding results\ntravis_uof[\"FULL_ADDRESS\"] = travis_uof[\"LOCATION_FULL_STREET_ADDRESS_OR_INTERSECTION\"] + \\\n\", \" + travis_uof[\"LOCATION_CITY\"] + \\\n\", \" + travis_uof[\"LOCATION_STATE\"]\n\n# Now let's assemble the 3 series we'll use to create our test df\nrand_addresses = travis_uof.loc[rand_addresskeys, \"FULL_ADDRESS\"]\nrand_state = travis_uof.loc[rand_addresskeys, \"LOCATION_STATE\"]\nrand_x = travis_uof.loc[rand_addresskeys, \"X_COORDINATE\"]\nrand_y = travis_uof.loc[rand_addresskeys, \"Y_COORDINATE\"]\n\n# And finally create the test df\nrand_address_table = pd.DataFrame({\"address\" : rand_addresses.values, \n                                   \"state\" : rand_state,\n                                   \"X_COORDINATE\": rand_x.values, \n                                   \"Y_COORDINATE\": rand_y.values})\n\n# Convert our rand_address_table to a rand_geodf (a GeoPandas df)\nrand_geodf_source = make_geodf_uof(rand_address_table, \"X_COORDINATE\", \"Y_COORDINATE\")\nrand_geodf_source.reset_index(drop = True, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13190db73785dd9a2d96b6e1d41d8940e5d42cac"
      },
      "cell_type": "markdown",
      "source": "### The result - what do we have?\n\n- We now have the <b>answer</b> to \"which co-ordinate reference system was used to compile this data?\" \n- We can also examine the contents of <b>rand_eval</b> to see which CRS's were our top contenders"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "6b36b4e396408bb8676e8299b838e47f797bba0a"
      },
      "cell_type": "code",
      "source": "# Run the find_crs function and then display our final answer\nanswer, rand_eval = find_crs(rand_geodf_source)\nanswer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "7f5d83aa1f95e4db0a091957904ea8838afc598d"
      },
      "cell_type": "code",
      "source": "# And also have a look at the top 5 options - notice that there are in fact 4 different projections\n# that would minimize the difference between our 'ground truth' co-ordinates obtained from Google\n# and our new projections based on our chosen CRS\nrand_eval.loc[\"avg\"].dropna().sort_values().head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a1272321340aeca250efb05d4197c77229f07220"
      },
      "cell_type": "code",
      "source": "# Let's now convert our Travis dataframe to a GeoPandas dataframe\ntravis_uof = make_geodf_uof(travis_uof, \"X_COORDINATE\", \"Y_COORDINATE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "8be066c83f80b76a33e731e64eda7c0282bb170d"
      },
      "cell_type": "code",
      "source": "# And then specify the CRS we identified as best fit\ntravis_uof.crs = {'init' : answer}\ntravis_shapes.crs = {'init' : answer}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "e9558ed30fec86f3cecbea84e54d1ef8e90e77ce"
      },
      "cell_type": "code",
      "source": "# After which we can convert to our standard\ntravis_uof = travis_uof.to_crs(epsg='4269')\ntravis_shapes = travis_shapes.to_crs(epsg='4269')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5cf818b8145b3033d24a8e8d20bdff7449cf2de1"
      },
      "cell_type": "code",
      "source": "# It would be nice to add labels to our data as a finishing touch so we'll get \"representative points\"\n# for each shape which we'll use to plot our labels later on\ntravis_shapes['coords'] = travis_shapes['geometry'].apply(lambda x: x.representative_point().coords[:])\ntravis_shapes['coords'] = [coords[0] for coords in travis_shapes['coords']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "722ddb6f9ea8bab0217b1e9ed2baba851e7226ff"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize = (12, 12))\n# Plot the ACS data\nax = travis_acs.plot(ax = ax, color = \"paleturquoise\", edgecolor = \"darkgrey\", linewidth = 0.5)\n# Add the shapefile data\ntravis_shapes.plot(ax = ax, column = \"DISTRICT\", cmap = \"viridis\", vmin = 1.8)\n# Add the use of force data\ntravis_uof.plot(ax = ax, color = \"orangered\", alpha = 0.2, markersize = 10) \n# Provide a legend\nuof_line = matplotlib.lines.Line2D([], [], color='orangered', markersize=120, label='use of force')\nhandles = [uof_line]\nlabels = [h.get_label() for h in handles] \nax.legend(handles=handles, labels=labels, fontsize = 10, loc='lower right', shadow = True)\n# Add a title\nfig.suptitle('Travis Texas, Use of Force by Police District', x = 0.5, y = 0.82)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a73b2bb69bd4d5d22f4d1efc8d665cad26dbde31"
      },
      "cell_type": "markdown",
      "source": "### Conclusion"
    },
    {
      "metadata": {
        "_uuid": "486763b82862c9c83c0738c76e0c35e492db361a"
      },
      "cell_type": "markdown",
      "source": "This is just a \"proof of concept\" type notebook, but with some additional effort to automate the above process for all departments and include robust error handling I think it might lay a good foundation! But let me know what you think - this is my first kernel submission on Kaggle, so be gentle :)... https://shotlefttodatascience.com/"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b24c848cf701d0789c060753cf602fb18cca1b35"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}